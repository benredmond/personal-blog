[
  {
    "id": "5",
    "slug": "autodidactic-universe",
    "title": "Spacetime as a Neural Network",
    "excerpt": "In 2021, physicist Lee Smolin, Jaron Lanier, and others published a paper with a bold claim: write Einstein's general relativity in a specific form, and the equations governing spacetime curvature correspond to the equations of a Restricted Boltzmann Machine. The implication: the very structure of spacetime itself might be fundamentally learnable.",
    "content": "What if spacetime itself - the fabric of our reality - could be formulated as a neural network?\n\nIn 2021, physicist Lee Smolin, Jaron Lanier, and others published a paper, [The Autodidactic Universe](https://arxiv.org/abs/2104.03902), with a claim: write Einstein's general relativity in a specific form (the Plebanski equation), and the equations governing spacetime curvature correspond to the equations of a Restricted Boltzmann Machine (RBM). \n\nI had to read that sentence three times.\n\nThe trick is to bridge the gap using a common language - matrices. In physics, N×N matrices are used to describe gauge fields and matter fields, and when N→∞, can be used to describe continuous spacetime and quantum fields. In machine learning, matrices represent the weights in the neural network, determining how information flows between the layers.\n\nHere's the correspondence:\n\n```\nPLEBANSKI GRAVITY                  RESTRICTED BOLTZMANN MACHINE\n───────────────────────────────────────────────────────\nQuantum matter fields           ↔   Network layers\nQuantum gauge, gravity fields   ↔   Network weights  \nEvolution of the law over time  ↔   Learning (updating weights)\n```\n\nThe implication: **the very structure of spacetime itself might _have been learned over time, and is fundamentally learnable._** Taking this further, the authors of the paper argue that the nature of our universe is one in which physical laws (like gravity) are learnable.\n\n*Note: I am not a physicist, so I recommend reading the original paper and/or talking to my [NotebookLM](https://notebooklm.google.com/notebook/59340775-bfce-45b2-96d0-20f0abed11b6) notebook to query the paper to gain a deeper understanding.*\n\n## The Caveat\n\nThis isn't an equivalence - the paper clearly states so.\n\nThe correspondence only works when you reduce continuous spacetime to finite matrices. Physical gravity requires taking those matrices to infinity (N→∞). The neural network at N→∞ is a different beast. \n\nDespite this, harmony between two domains that have no business rhyming is still remarkable.\n\n## How the Universe Learns\n\nBut what does it even mean for a universe to \"learn\"?\n\nThink of a river carving a canyon. The water molecules are **fast variables** - they rush and swirl, their paths dictated by the riverbed. The canyon walls are **slow variables** - to the water, they appear as fixed \"laws\" governing where it can flow. But in truth, the canyon wall is anything but fixed. Zoom out to geological timescales, and we see that it evolves over time, and this evolution is dictated by the movement of the water molecules. \n\nThe canyon is what the authors call a **consequencer** - a structure that allows the past to impact the future by accumulating information over time. As they put it: \"The consequencer is no more and no less than the information that must change when learning occurs\".\n\nIn neural networks, we can see weights serving as the consequencer. While training, billions/trillions of data points flow through the system (fast variables), while the weights (slow variables) change over time to cement patterns. In each individual run, to the data, the weights appear to be (and are!) fixed constructs governing how information flows. Zoom out to the  epoch level though, it becomes clear that the \"laws\" (weights) are learning over time, and encoding information into its structure.\n\nThe authors of the paper suggest that the structure of the universe works similarly, with geometry itself as the consequencer.\n\n```\n            creates\n    MATTER ─────────► GEOMETRY\n       ▲                  │\n       │                  │\n       │    tells how     │\n       └───── to move ────┘\n```\n\nMatter shapes geometry; geometry constrains matter; the cycle repeats. Each loop encodes information from the past into the structure of spacetime itself.\n\n## An Autodidactic View of the Universe\n\nIf our universe truly is autodidactic (self learning), it may help answer a crucial question in physics: **why do we have the laws that we do**?\n\nFor those with an Anthropic viewpoint, an argument may be: the fact that we're here to observe the laws is sufficient reason alone - if they were different, we wouldn't exist to even ask the question.\n\nAlternatively, the Autodidactic Universe answers that laws exist because the universe **learned them over time** as an answer to a cost function - one that prioritizes stability or variety. \n\nThe \"over time\" part is crucial. In Smolin's _Time Reborn_, he argues, against modern physics,  for the reality of time. If instead, as Einstein proposed in Special Relativity, time is unreal, then the concept of learning over time fundamentally doesn't make sense. \n\n## What this can tell us about AI\n\nIf learning is a fundamental to reality itself - if the universe has been doing \"gradient descent\" for the past 13 billion years - then perhaps the tools we're building might not be as artificial as the name suggests.\n\nAI certainly isn't (and can never *truly* be) human - no matter how much tech executives would like you to believe otherwise.\n\nBut just because it isn't *human*, doesn't mean it isn't *natural*.\n\nThe tools (like Claude), which helped me research this topic and proofread this post - that we're collectively spending trillions to build - may be yet another case of humanity \"rediscovering\" systems that Nature has honed over eons, morphing them for our economic and societal gain.\n\nThis raises questions we're not yet asking: What can physics' unsolved problems (e.g. quantum gravity) teach us about scaling AI? Should AI architecture evolve to mirror cosmological principles?\n\nMight the next big leap in the AI arms race come from cosmological physics instead of computer science? Time will tell.\n\n— Ben\n\n---\n## References\n\n- [The Autodidactic Universe](https://arxiv.org/abs/2104.03902) and associated [NotebookLM](https://notebooklm.google.com/notebook/59340775-bfce-45b2-96d0-20f0abed11b6)\n- Smolin's _Time Reborn_ (2013) for general readers on evolving laws. Fascinating read to challenge our assumptions about time and the universe as a whole\n- Kurzgesagt [video](https://www.youtube.com/watch?v=zozEm4f_dlw) about JWST discoveries causing a \"crisis\" in cosmological physics that inspired this rabbit hole",
    "author": "Ben Redmond",
    "category": "Physics",
    "tags": ["AI", "physics", "cosmology", "neural networks", "Smolin"],
    "date": "2025-12-29T10:00:00Z",
    "readTime": 8
  },
  {
    "id": "4",
    "slug": "one-agent-isnt-enough",
    "title": "One Agent Isn't Enough",
    "excerpt": "Agentic coding has a problem - variance. What if single-agent runs are leaving performance on the table by design? Due to the stochastic nature of LLMs, each agent run has slight variations. Even with the same context, one session might land near the peak, another somewhere in the middle.",
    "content": "Agentic coding has a problem - variance. What if single-agent runs are leaving performance on the table by design?\n\nDue to the stochastic nature of LLMs, each agent run has slight variations. Even with the same context, one session with an agent might land near the \"peak\" of where we could expect it to (rolling a 20 on a d20), and another session might land somewhere in the middle of the narrowed probability curve (rolling a 10 instead).\n\nIn [Part 1](https://benr.build/blog/intelligence-without-information) I talked about my mental model around context engineering: the goal of context engineering is to shift the **probability distribution** of LLM responses, where the \"probability distribution\" is the space of all possible results from the LLM.\n\nIn this piece, I'll talk about extending that mental model to adjust for the fact that we can easily and (relatively) cheaply trigger parallel agent runs.\n\nThe goal of context engineering is more than just decreasing standard deviation and improving the mean quality of the probability distribution - it's also to ensure reliable convergence to a local maximum.\n\n## The Limitations of Context Engineering\n\nUsing context engineering best practices (prompt engineering, reference to relevant documentation, targeted addition of relevant tools, skills, etc.) to shift the probability distribution handles the first-order problem: reduce the likelihood of bad outcomes, and raise the floor - improving the average quality of responses.\n\nBut it doesn't solve the *exploration* problem. Even within the improved distribution, there are multiple paths the agent can take to solve the problem. Some are decent. Others are optimal. A few will make you want to slam your head into the keyboard. A single agent run picks one path. You don't know if it's the peak, or just high enough satisfy you.\n\n## The Second Mental Model\n\n```\nSOLUTION LANDSCAPE\n\nSingle Agent:                        Parallel Agents + Synthesis:\n\n   ▲                                    ▲\n   │      ╱╲                            │      ╱╲\n   │     ╱  ╲    ╱╲                     │     ╱  ╲     ╱╲\n   │    ╱    ╲  ╱  ╲                    │    ╱ ②  ╲  ╱   ╲\n   │   ╱      ╲╱    ╲   ╱╲              │   ╱       ╲╱  ④ ╲   ╱╲\n   │  ╱    ①         ╲╱   ╲            │  ╱   ①            ╲╱   ╲\n   │ ╱                      ╲           │ ╱        ③    ⑤        ╲\n   └────────────────────────────►       └────────────────────────────►\n\n   You get what you get               Synthesizer picks ② as winner\n```\n\nThe solution to the problem is parallel agents (and lots of tokens).\n\nWith parallel agents, we take a \"sample\" multiple times (i.e. multiple runs of the same / similar prompt), explore different peaks, and use the findings from the group to converge on the best solution. We're able to hedge our bets, and using the knowledge of the crowd, consistently get more insight out of the LLM. \n\nWhy does this work? I'm not making $100M at Meta as an AI researcher so I can't answer - but I'll do my best to speculate. \n\n**Multiple Samples**\nThis is the main one that I've been mentioning. Five agents = five independent samples. You're not relying on a single path and some luck to find the peak.\n\nA single agent run might settle on a suboptimal solution - a local minimum that works but isn't great. It found something functional and stopped exploring. Parallel agents with independent starting points can escape these traps. They explore different regions of the problem space, pushing past mediocre solutions to find better ones. The convergence pattern reveals when multiple paths lead to the same superior approach.\n\n**Different Starting Points**\nClean context windows mean no anchoring bias. Each agent explores from a fresh perspective. \n\n**Validation Through Repetition**\nWhen two agents independently suggest the same approach, that's evidence it's a local maximum. When all agents diverge, you need more constraints.\n\n**The parallel structure transforms coding agents from sampling from a single random draw into being a guided search for peaks.**\n\n## How I Use Parallel Convergence\n\nI use parallel convergence primarily in two ways / phases that fall into workflows of\n\n1. Generating multiple solutions to a problem\n2. Gathering information from multiple sources about a problem\n\nHere's how it works:\n\n*Note that I primarily use Claude Code, which supports subagents via an orchestrator pattern, meaning that one main agent spawns subagents, and later synthesizes the results*\n\n```\nPARALLEL CONVERGENCE WORKFLOW\n\n         Phase 1                    Phase 2\n         GATHER                     SOLVE\n\n         A   B   C                  X   Y   Z\n         │   │   │                  │   │   │\n         └───┼───┘                  └───┼───┘\n             ▼                          ▼\n         synthesize ──── plan ───▶ synthesize ──▶ execute\n```\n\n**Generate multiple solutions to a problem**\n\nIn this workflow, I'll use multiple agents to come up solutions to the same problem. The goal here is to de-risk the fact that any one agent may come up with a sub-par solution. \n\nWhen spinning up the subagents, Claude may assign them different angles to approach the problem at, allowing main Claude to explore more of the problem space. \n\nFor example, if I'm debugging why a modal renders behind everything despite z-index: 9999 (we've all been there), Claude might approach the problem from a data flow, React hooks, and component layering perspectives.\n\nClaude then synthesizes, validates, and proposes a solution based on the outputs from all subagents. If 3/5 subagents came up with a similar solution, then it is more likely that this solution is what we want, and we should move forward with it. \n\nI most commonly use this in debugging cases, but it's also been useful in the planning phase of a more complicated task. \n\n**Gather information about a problem**\n\nAs part of my planning workflow in Claude Code, I dispatch multiple intelligence-gathering subagents. Here are some examples of the agents I'll use:\n\n- Agent A: scan git history (what patterns exist?)\n- Agent B: search local documentation (what's been tried before?)\n- Agent C: map code paths (what interfaces are available?)\n- Agent D: analyze test coverage (what validation already exists?)\n- Agent E: identify constraints (what are the boundaries?)\n- Agent F: find risks (what do we need to watch out for?)\n- Agent G: web research (what do online resources say?)\n\nYes, seven agents is excessive. It is! I won't unleash all seven (that's chaos) - but having the full menu available matters.\n\nEach explores independently. Each has a chance to discover different information about the problem approaching it from a slightly different perspective. Different goal here: while in the previous one, we are dispatching agents to discover solutions to the same problem, in this case, we are dispatching agents to find distinct, but complementary information related to the problem.\n\nWith this information in hand (and context sufficiently primed), Claude can then proceed with making a plan for solving the problem. (If you want to double down on parallelism, you can also use parallel agents for planning).\n\n## What Convergence Looks Like\n\nHere's an example from an \"AI hedge fund\" project I'm working on for model evaluation. \n\nThe problem: the AI could articulate detailed failure modes (good), but claimed Sharpe ratios that would make Renaissance Technologies jealous (bad). It had the *form* of institutional risk documentation without the *calibration* of realistic return expectations. I needed to update the prompts to address this case.\n\nI launched 4 parallel intelligence-gathering agents:\n\n- **Agent A (Intelligence Gatherer)**: Found similar past tasks related to commits adding calibration to edge scoring\n- **Agent B (Extracting Patterns from Codebase)**: Found the same edge scoring calibration pattern in the codebase, noted it was \"proven effective\"\n- **Agent C (Git Historian)**: Found the exact same commit history, described 3 \"calibration improvement waves\" over 7 weeks\n- **Agent D (Web Researcher)**: Found Ken French Data Library and AQR research with actual factor premium numbers (momentum: 5-8% annually, quality: 2-4%)\n\nAll four agents, exploring from completely different angles (pattern database, codebase analysis, git history, web research), converged on the same solution: add calibration guidance using the existing Anti-Patterns section format, grounded in historical factor data.\n\nEven better: Agent A initially suggested a 60-line dedicated section. But when Claude synthesized all the findings, the convergence pattern showed a simpler path - a 5-line addition to the existing Anti-Patterns section would achieve the same goal without context bloat.\n\nHow does Claude actually synthesize? Honestly, I don't control that directly it's part of Claude Code's orchestrator pattern. But I can see what happens: it weights agreement heavily, surfaces outliers worth considering, and, critically, tends toward simpler solutions when convergence supports it (with additional prompting pushing for simplicity). That's how a 60-line suggestion became 5.\n\nThe convergence told me two things:\n1. The solution was validated (4 independent explorations → similar conclusions)\n2. The minimal version was sufficient\n\nThe cost was ~10 minutes of parallel agent time, and maybe 200k tokens total. My Claude \nCode usage limits weep, but the payoff was a high-confidence solution with evidence from multiple independent sources, plus the discipline to keep it simple.\n\nIf this sounds excessive for a 5 line change, it is! That's kind of the point.\n\nEven for a ~5 line prompt change, it was worth grounding those 5 lines in past decisions, web research, and agent consensus.\n\n## When NOT to Use This\n\nThe multi-agent approach doesn't come without its drawbacks:\n\n- Token use\n- Context bloating in main agent from the additional information\n- Time waiting for agents\n\nA single agent is largely sufficient for well-defined tasks, simple changes, or easy bugs. In other cases, I start to consider using the parallel workflow.\n\n## From Random Walk to Guided Convergence\n\n```\nWHAT CONVERGENCE TELLS YOU\n\n     Agents Agree                     Agents Diverge\n     ─────────────                    ───────────────\n\n       \"caching\"                        \"caching\"\n       \"caching\"                        \"rewrite DB\"\n       \"caching\"                        \"add index\"\n           │                                │\n           ▼                                ▼\n        EXECUTE                     1. Tighten constraints\n                                    2. Ask user for opinions on path forward\n```\n\nPart 1's model: better context engineering → better distribution → better average outcomes\n\nIn this model: better context → better distribution → parallel exploration → convergence validation → optimal outcomes reliably\n\nTo summarize: **Context engineering creates the right distribution. Parallel convergence finds the peaks within it.**\n\n## Next Steps\n\nNext time you're debugging a tricky issue, spin up 3 parallel agents, and see if they're able to find something that 1 agent alone couldn't.\n\nThis probably sounds more systematic than it felt. In practice, it's a lot of \"let's see what happens\"! If you have learnings from a similar workflow, or thoughts on the article, reach out to me - I'd love to discuss what you're doing!\n\n— Ben\n\n_Part 2 of a series on context engineering and building with AI coding agents. Part 1 introduced probability distributions and information architecture. In subsequent pieces, I'll go into specifics on my workflow, and what I've learned from building KOUCAI._",
    "author": "Ben Redmond",
    "category": "Thoughts",
    "tags": ["AI", "context engineering", "agents", "Claude Code", "parallel agents"],
    "date": "2025-12-13T00:09:43Z",
    "readTime": 12
  },
  {
    "id": "3",
    "slug": "intelligence-without-information",
    "title": "Intelligence Without Information Is Squandered",
    "excerpt": "Claude confidently pointed me to a code path in MongoDB's massive infrastructure codebase. I spent an hour investigating it before realizing something was off. The path worked, technically. But there were better approaches - ones the agent had completely missed.",
    "content": "Claude confidently pointed me to a code path in MongoDB's massive infrastructure codebase. I spent an hour investigating it before realizing something was off. The path worked, technically. But there were better approaches - ones the agent had completely missed.\n\nI was implementing a new communication mode between our control plane and data plane. Enterprise scale, hundreds of thousands of lines of code, the kind of codebase where AI agents supposedly fall apart. I'd asked Claude a simple question: \"How can I send this config to the data plane?\"\n\nIt pointed me to a break-glass path that was meant for emergency scenarios. Technically correct, but not structurally sound, and missed a change that required 4 lines to do the same thing in a much more elegant way.\n\nThe model had everything it needed except the one thing that mattered: context to help it determine which approach was actually best.\n\n## The Mental Model\n\n```\nPROBABILITY DISTRIBUTION OF OUTCOMES\n\nWithout context engineering:\n    │   ╱╲\n    │  ╱  ╲\n    │ ╱    ╲___\n    └──────────────>\n     (wide spread, low mean quality)\n\n\nWith context engineering:\n    │     ╱╲\n    │    ╱  ╲\n    │   ╱    ╲\n    └──────────────>\n     (narrow spread, high mean quality)\n```\n\nAll AI output is probabilistic. You're sampling from a distribution of possible responses - some good, some... not quite.\n\nThe goal of context engineering is to shift that distribution. Without good context, you get a wide spread, with lower mean quality. With good context, the curve tightens and moves to the right - narrow spread, high mean quality.\n\nThere's no way to guarantee a specific output. Instead, your objective is to increase the probability of a good-enough output.\n\n## Context Limited\n\nMost developers using AI coding Agents (Claude Code, Cursor, Copilot, etc.) treat AI agents as if they are glorified junior engineers, or a better tab auto-complete. \n\nVague task, simple prompt, hit enter, pray.\n\nWhat's *so powerful* about these Agents is that they are Swiss Army Knives full of tools - grep, file read, bash commands, web search, examine git history. Additionally, advances in intelligence has made it possible for them to run for 10s of minutes at once without stopping.\n\nThe model capability is already there. The \"intelligence\" is already there. \n\nWhat's missing is the information architecture - the engineering rigor of writing code with these tools. Or, as Simon Willison calls it, [Vibe Engineering](https://simonwillison.net/2025/Oct/7/vibe-engineering/).\n\n > Most agent failures are not model failures anymore, they are context failures\n\n *https://www.philschmid.de/context-engineering*\n\nIn the post-AI world, the bottleneck has shifted. Writing code is easy now, however writing *good* code is still hard. To write good code with Agents, the #1 question vibe engineers should be asking is \"how well can I curate the information environment that the Agent operates in?\"\n\nThe quality of the tokens output by the agent is almost entirely dependent on the quality of what goes in. Or, garbage in, garbage out.\n\nMost people still view AI as a black box - type something in, get something out. But you have far more control over the output than you realize. \n\nContext engineering, as put by [Shopify CEO Tobi Lutke](https://twitter.com/tobi/status/1935533422589399127), is \n> the art of providing all the context for the task to be plausibly solvable by the LLM.\n\nIn other words, the craft of deliberately architecting what and when information enters the AI's context window, and what stays out, to maximize the probability of getting the output you actually want.\n\n## What's Actually in the Context Window\n\n![Diagram showing the layers of context in an LLM interaction](https://www.philschmid.de/static/blog/context-engineering/context.png)\n*Source: [philschmid.de - Context Engineering](https://www.philschmid.de/context-engineering)*\n\nEvery interaction uses multiple layers: system prompts, project config, tools, artifacts, and your prompt. The amateur optimizes the last layer. The professional architects all of them. (We'll dive deep into specific techniques to optimize these layers in later parts.)\n\nNext time you give Claude Code a task, run `/context` first. See what's actually in the context window, and think about what you need, and what you don't.\n\n## When Context Matters\n\nNot every task needs sophisticated context engineering. \n\nSimple tasks work with simple context - error message + file + ability to test = ability to fix a compile error, or a basic unit test. It'll figure it out. (It's pretty dang smart...)\n\nImplementing a feature as part of a larger project? Now, the context engineering requires deliberate architecture, and thought about what to include, and when.\n\nThe divide between these two approaches to context is fuzzy - however, as I became more familiar with the practice of context engineering, I began to intuit when to use the low touch vs high touch frameworks.\n\nAn example from  [KOUCAI 口才](https://koucai.chat)  (AI penpals for practicing Chinese):\n\nThe Task: \"The marketing website (koucai.chat) is making unnecessary API calls to the backend, slowing down page load. Please fix this.\"\n\n**Naive Approach** (What Could Have Happened):\n\nJump straight to implementation:\n- Add a quick if (hostname === 'koucai.chat') return; check\n- Gate the API call\n- Ship it\n\nThis might work. But you'd miss:\n- An identical pattern already exists in pages/index.jsx\n- A previous commit (cf3f229) partially implemented this exact fix but incompletely\n- The race condition between domain detection and initialization\n- The loading state edge case that causes infinite spinners\n- Test coverage requirements and existing mocking patterns\n\nResult: Ship a \"working\" fix that breaks in production with infinite loading spinners, requires a hotfix, and doesn't follow established patterns.\n\n**Context-Engineered Approach** (What Actually Happened):\n\nPhase 1: Systematic Intelligence Gathering\n\nInstead of coding immediately, Claude dispatched parallel research agents:\n\n1. Search task history\n    - Discovery: pattern for CONDITIONAL_WRAPPER_RENDERING exists\n    - Discovery: Commit cf3f229 partially implemented this pattern\n    - Discovery: Pattern was applied to rendering but NOT initialization (the actual bug)\n2. Codebase Analysis Agent: Map existing domain detection implementations\n    - Discovery: Pattern exists in 3 files with proven loading state guards\n    - Discovery: pages/index.jsx has the complete reference implementation\n3. Git Archaeology Agent: Trace evolution of guest session logic\n    - Discovery: Recent commit added domain detection but left race condition\n    - Discovery: Similar race condition fixes exist in commit 744d471\n4. Risk Analysis Agent: Predict failure modes\n    - Prediction: tests may fail due to missing window.location mocks\n    - Result: All 15 tests passed without modification (good prediction!)\n\nPhase 2: Mandatory Architecture Artifacts\n\nBefore writing ANY code, Claude created:\n\n1. Chain of Thought Analysis: Why does the current implementation exist? What's the git history? What breaks easily?\n2. Tree of Thought: Generated 3 fundamentally different solutions:\n    - Solution A: Sequential useEffect with state flag (proven pattern)\n    - Solution B: Synchronous detection in useState initializer (SSR risk)\n    - Solution C: Lift to parent component (over-engineering)\n    - Winner: Solution A (minimal change, proven pattern, 30min estimate)\n3. YAGNI (you aren't gonna need it) Declaration: Explicitly excluded:\n    - Centralized useIsMarketingDomain hook (future refactor)\n    - Performance optimization via memoization (unnecessary)\n    - Loading skeleton UI (overkill for instant detection)\n\nPhase 3: Pattern-Guided Implementation\n```\n// Not guessing - copying proven pattern from pages/index.jsx lines 48-67\nconst [isDomainDetected, setIsDomainDetected] = useState(false);\n\nuseEffect(() => {\n// Detection logic\nsetIsDomainDetected(true); // Signal completion\n}, []);\n\nuseEffect(() => {\nif (!isDomainDetected) return; // Wait for detection\nif (isMarketingDomain) return; // Skip marketing\n// Safe to initialize\n}, [isDomainDetected, isMarketingDomain]);\n```\n\n\nPhase 4: The Context Engineering Payoff\n\nFirst implementation had a bug (infinite loading spinner). But because Claude had:\n- The complete pattern context from pages/index.jsx\n- Understanding of the loading state architecture\n- Knowledge that isInitializing starts as true\n\nClaude immediately recognized the anti-pattern:\n```\nif (isMarketingDomain) return; // ❌ Forgot to clear isInitializing!\n\n// Fixed!\nif (isMarketingDomain) {\n    setIsInitializing(false); // ✅ Clear blocking state\n    return;\n}\n```\n\n**The Difference**\n\n| Naive Approach                   | Context-Engineered Approach                   |\n|----------------------------------|-----------------------------------------------|\n| 10 minutes to \"working\" code     | 35 minutes to proven solution                 |\n| Ships with infinite spinner bug  | Catches bug immediately via pattern knowledge |\n| Doesn't follow existing patterns | Reuses CONDITIONAL_WRAPPER_RENDERING          |\n| No learning from past mistakes   | Discovered cf3f229 was incomplete             |\n| Reinvents the wheel              | Copies proven pattern from pages/index.jsx    |\n| Unknown unknowns remain unknown  | Parallel agents surface hidden complexity     |\n\n**Takeaway**: Context engineering isn't about feeding more tokens to the model. It's about systematically assembling the right context before attempting implementation.\n\nThe 25 minutes spent on intelligence gathering prevented:\n- Reinventing a solution that already existed\n- Missing the edge case that causes infinite loading\n- Breaking from established codebase patterns\n- Incomplete fixes like the previous commit\n\nUsing the same model, and similar prompt length, will lead to completely different outcomes. The difference is I gave the agent the information needed to make informed decisions. I architected the context.\n\nThe workflow forced Claude to ask: \"What similar work exists? What patterns apply? What failed before?\" before writing a single line of code. That's context engineering.\n\nBe intentional about *every token* of context you give to the model.\n\n## When Context Breaks down\n\nClaude starts the session perfectly. It reads the spec, understands the requirements, makes a sensible plan.\n\n100,000 tokens later, things start to break down. It's implementing features not in the spec. It's not writing tests, or faking results to get tests to pass, it's explicitly not following instructions\n\nThis is **context rot**.\n\nIt's easy to assume more context is always better. It's not. \n\n> As its context length increases, a model's ability to capture these pairwise relationships gets stretched thin, creating a natural tension between context size and attention focus. Additionally, models develop their attention patterns from training data distributions where shorter sequences are typically more common than longer ones. This means models have less experience with, and fewer specialized parameters for, context-wide dependencies.\n\n*https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents*\n\nLoad your entire codebase, all specs, complete git history, and every document into the context window, and the model's performance degrades. It can't sustain the enormous amount of context. The signal-to-noise ratio collapses. The AI starts making spurious connections between unrelated information. It gets distracted. It hedges. Its outputs become vague and uncertain.\n\nThe 10,000 token handy MCP for using JIRA? Pretty valuable when doing research, right? (Claude is certainly better than me at JQL...). However, this is pure context rot when debugging a UI component. These tokens aren't just wasted - they're actively downgrading the performance of the model.\n\nContext engineering is just as much about removal as addition.\n\n## The Taste of Curating Context\n\nHow do you actually practice context engineering? Here's my journey:\n\nWhen I first built [KOUCAI 口才](https://koucai.chat), I initially interacted with Claude in fairly basic ways (implement this component, fix the bug, etc.). Claude would sometimes get it right, sometimes not. I was playing roulette.\n\nAs I learned more about AI coding, I developed a more sophisticated workflow with five phases: gather intelligence, plan, execute, validate, review. The intelligence and plan phases use context engineering to pull in relevant information so that the execution phase can make the correct changes. \n\nThis is how *I* approach context engineering. You'll develop your own workflow. The specific details and prompts don't matter for now. What matters is knowing that **every context decision is a judgement call by you, the human**.\n\nWhich patterns matter enough to encode in your system prompt? Which tools are worth the token cost for this task? What historical context helps the agent make informed choices? When does additional context add signal versus noise?\n\nIn [The New Math of Building with AI,](https://benr.build/blog/new-math-ai) I argued that when action becomes cheap, taste becomes king. Context engineering is how you operationalize that taste. It's how you apply judgment when working with AI.\n\nContext curation is taste in action.\n\n## What This means for You\n\nModel capabilities are accelerating. Agents are getting smarter and are able to work for longer. Whatever comes next (Gemini 3, Opus 4.5, etc.), they'll be smarter, faster, more capable, than anything that came before.\n\nBut intelligence without information is squandered.\n\nEngineers who succeed with AI agents aren't necessarily the ones writing clever prompts. They're the ones who:\n\n- Build system prompts that encode their team's standards and patterns\n- Create workflows that generate reusable, high-quality context artifacts that compound over time\n- Choose MCP tools strategically based on task requirements\n- Know when to add context and when to remove it for clarity\n- Compress complex information into forms agents can efficiently use\n\nThese are all judgement calls and require craft to get right.\n\nAI made action cheap. Context engineering is how you apply the judgment that determines whether that action produces something worth shipping.\n\n## Start here\n\nStart here: Run `/context` in your next Claude Code session. Look at what's loaded. For each piece of context, ask: \"Is this helping or cluttering?\"\n\nThis is the craft. The tooling around AI coding is still early - you'll write CLAUDE.md files and custom slash commands because the platforms haven't caught up yet. That'll improve. The skill of knowing what context matters won't.\n  \nClaude Code is waiting for your command. The question is whether you're ready for it.\n\n— Ben\n\n_This is Part 1 of a series on context engineering and building with AI coding agents, like Claude Code. In later parts, I will go into specifics into certain parts of the context engineering workflow (slash commands, specs/plans, parallel agents, etc.)._\n## Appendix\n\nThese are a few articles I enjoy about context engineering. Please read them!\n\nhttps://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents\nhttps://www.philschmid.de/context-engineering\nhttps://github.com/humanlayer/advanced-context-engineering-for-coding-agents/blob/main/ace-fca.md",
    "author": "Ben Redmond",
    "category": "Thoughts",
    "tags": ["AI", "context engineering", "software engineering", "agents", "Claude Code"],
    "date": "2025-11-04T10:00:00Z",
    "readTime": 15
  },
  {
    "id": "2",
    "slug": "new-math-ai",
    "title": "The New Math of Building with AI",
    "excerpt": "The sixteenth draft of my Chinese dictionary lies abandoned in a folder deep within my repo. Two weeks of work, genuinely good code, completely functional. Pre-AI me would have shipped it just to justify the time spent. Post-AI me deleted it and started building something better. That folder of abandoned ideas tells the real story of how AI changes everything.",
    "content": "The sixteenth draft of my Chinese dictionary lies abandoned in a folder deep within my repo. Two weeks of work, genuinely good code, completely functional. Pre-AI me would have shipped it just to justify the time spent. Post-AI me deleted it and started building something better. That folder of abandoned ideas tells the real story of how AI changes everything.\n\nThis is the new math of being a builder: idea exploration is free, but shipping still takes weeks. With AI, we're not simply moving faster, we're taking better bets.\n\n*I'll illustrate this through my experience building koucai.chat, a Chinese language learning app, from zero to production in a few months. I am a professional software engineer, so I'm not using AI to replace my coding knowledge, but instead to allow me to achieve more.* \n\nHere's what changed.\n\n## Moonshots\n\n```\n      SCARCITY                    ABUNDANCE\n\n           ●                      ● ● ● ● ●\n                                  ● ● ● ● ●\n      precious                    ● ● ● ● ●\n      resource                    ● ● ● ● ●\n                                  ● ● ● ● ●\n           ↓                           ↓\n\n      \"protect                    \"explore\n       the idea\"                   everything\"\n```\n\nBy making action cheap, AI allows us to take bigger risks and chase moonshot ideas. Cheap action gives us VC logic - make ten bets, expecting eight to fail.\n\nBut, that raises the question, shouldn't someone with a good product sense *already know* where to apply their efforts, have an idea of what they should be building? \n\nCertainly, we *should* know what to do, but in practice, we don't know how hard it will be, or what the value will be, until we actually do it *(the reason that correctly estimating timelines is a rare and esteemed skill)*.\n\nSome examples from Koucai are:\n\n**Building a Chinese-English dictionary**\n\n- Online free dictionaries didn't have the level of detail I wanted\n- I wanted to experiment using AI to fill in the existing dictionary with extra information such as particle, example sentences, collocations, etc.\n  - (I know - using AI for language learning is a bad idea. However, I theorized with a high enogh (95%+) accuracy, the benefits would outweight the drawbacks)\n- I built out a prototype for this, but I realized that it didn't have the level of accuracy and quality that I required\n\nAfter two weeks of building this (in parallel with other features), I had three options:\n\n1. Ship the 80% solution (fast, but dictionary quality would degrade the user experience)\n2. Spend another month reaching 95% (production-quality, but at cost of delaying other core features)\n3. Abandon it and use existing dictionaries (pragmatic, but compromised on my needs)\n\n  I picked #3. If I had written everything by hand, I likely would've picked #2, as the sunk cost was real, and 'almost done' feels too close to quit.\n  \n  But, when prototyping is cheap, sunk costs stop mattering. The real question became: Is this the best use of the next month? It wasn't.\n\n\n**Implementing a scenario feature**\n\n- I wanted to implement a feature allowing users to place the AI character in different \"scenarios\"\n- I wasn't sure how this would look, so I ideated on the feature a bit, and then set my buddy Claude out to build it\n- The result: a feature that significantly improved the 'fun' factor of the app, built in days instead of never leaving my brain\n\nWithout cheap action, I would have been much less likely to take the risk of investing time into this unknown idea\n\nFailure is now affordable. Since action is cheap, so is failure. However, **the learnings gained from failure are still real.** \n\n## Compression of Action\n\n```\nTRADITIONAL:\n\n    Think ──────[weeks]──────> Do ──────[weeks]──────> Learn\n      ↑                                                   │\n      └────────────────[iterate slowly]───────────────────┘\n\n\nWITH AI:\n\n    Think ─[hours]─> Do ──[days]─> Learn ┤\n      ↑                                  │\n      ├──[hours]──> Do ──[days]──> Learn ┤  (parallel paths)\n      │                                  │\n      └──────[rapid iteration]────────-──┘\n```\n\nThe most dramatic effect AI has had on the Thinking <-> Doing cycle is the compression and parallelization of action. \n\nThe time it takes to make *visible* changes, especially for greenfield projects, is shortened due to the extensive knowledge and crazy speed that AI can write new code. Additionally, agents can be parallelized, scaling out work, where the only limit is the cognitive overload of managing the agents.\n\nWith AI, action has become cheap. Necessarily, this places more weight on the longer-tail cycle of Thinking, changing the calculus of being a software engineer. Each idea has the *potential* to become something real and tangible - all that's required is the desire to bring it into reality.\n\n## Doing good things is still hard\n\nAI compresses the Thinking <-> Doing cycle, but it doesn't actually make doing good things *easy*. Even though AI is fast at writing code, and can do so in parallel, **building software with professional standards using AI isn't actually always faster.** Thankfully for those of us in the industry, software engineering is still a delicate and difficult craft, and, despite claims to the contrary, continues to require a high level of coordination and thought by a human.\n\nGetting to demo quality takes days. Getting to production quality still takes months. The difference is now, we have a better idea of if those weeks are worth it.\n\nThis is due to the lack of a world model in LLMs. Their view of the world is so simplistic that it isn't able to have the 10000ft view that is required to build software systems. AI excels at generating code but stumbles on the invisible: system constraints, cross-component dependencies, and the accumulated decisions that shape mature codebases\n\nWhere AI *does* help is the parallelization of your skill. Each AI agent can be working on independent task, and you, as the orchestrator, can coordinate the agents and steer them in a correct direction\n\nAn example is implementing a guest user flow. I dislike when apps require you to have an account before trying it out, so I decided to allow un-authed users to trial Koucai. Doing so sounds easy, but in reality it doubles the amount of surface area for bugs related to auth, both in the frontend and backend. Implementing the feature took about a week of refinement, testing, and lots of back-and-forth with the AI before it was in an acceptable state. \n\n## Thinking is cheap now?\n\n```\nYOUR IDEA                          YOUR IDEA + AI\n    ┌─────────┐                       ┌─────────────────┐\n    │   ∙     │                       │  ∙   ∙   ∙      │\n    │         │  ──────[ AI ]──────>  │    ∙   ∙   ∙    │\n    │    ?    │                       │  ∙   ✦   ∙      │\n    └─────────┘                       │    ∙   ∙   ∙    │\n     (fuzzy)                          └─────────────────┘\n                                      (explored & refined)\n```\n\nWhen we can offload our thinking to AI, what remains uniquely human?\n\nAI takes our ideas, and shifts them in new and unexpected ways via the stochastic nature of the LLM. Taste is knowing which path to take when AI shows you ten.\n\nThere are two paths that can be taken when working with AI:\n\n1. Using AI to replace your ideas\n2. Using AI to enhance your ideas\n\nSomeone who is doing #2 can use AI as an idea validator, expander, enhancer, and thought partner.\n\nI knew design was critical for Koucai. To me, it's not simply a product. I'm building somewhere for people to *go* online. Form and function are intertwined. I wanted something beautiful. Specifically, I wanted Bauhaus principles (geometric, functional, bold) but couldn't figure out how to reconcile that with minimalism.\n\nI gave AI my constraints and references, design aesthetics I admired, the tension between bold and minimal. It generated multiple directions. My first attempt used aggressive shadows and heavy contrast, which was technically Bauhaus, but too loud for an interface people use daily. After iterating through variations, I landed on neo-Bauhaus × 間 (ma): geometric structure with intentional negative space, bold where it matters and restrained everywhere else.\n\nThe framework stuck. Now when I face design decisions, I have a lens: Does this honor geometric clarity? Does it use negative space intentionally? Is the boldness purposeful? AI helped me explore the space between my constraints, but I had to recognize the right answer when I saw it, and know when the first answer (aggressive shadows) was wrong.\n\nThis is the paradox: AI makes idea generation cheaper, but judgment becomes more valuable. You need taste to navigate the expanded possibility space. Which of these ten directions actually serves your users? Which honors your vision vs just looking cool? Which is bold enough to differentiate but restrained enough to live with?\n\nTaste becomes king in a world run by AI.\n\n## What this means\n\nEngineers who succeed in an AI world aren't necessarily the fastest coders, greatest prompters, or hardest workers - they're the ones who can orchestrate multiple AI agents while retaining coherence, curate ideas ruthlessly, and know when 'good enough' truly is good enough.\n\nWe're entering an era where the cost of being wrong approaches zero, but the value of being right remains unchanged. The engineers who'll thrive understand this new math: build ten things, ship three, learn from all ten.\n\nThe real divide isn't between those who use AI and those who don't, but between those who maintain their judgment/thinking and those who outsource it.\n\nWhich will you be?\n\n## Advice \n\nOpen your backlog. Pick an item that you didn't pick up because it seemed too complex or wasn't important enough, and give yourself 4 hours to prototype it with AI.\n\nYou'll either prove it's impossible, or discover it's not.\n\nEither way, you've converted a vague idea of the problem into tangible knowledge. That's the real value. \n\n— Ben\n\n*Articles I enjoyed while thinking about this post:*\nhttps://dcurt.is/thinking\nhttps://yosefk.com/blog/llms-arent-world-models.html",
    "author": "Ben Redmond",
    "category": "Thoughts",
    "tags": ["AI", "software engineering", "product development", "taste"],
    "date": "2025-10-16T00:00:00Z",
    "readTime": 12
  }
]
